{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3d0169",
   "metadata": {},
   "source": [
    "## Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515fc451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Arerê</td>\n",
       "      <td>/ivete-sangalo/arere.html</td>\n",
       "      <td>Tudo o que eu quero nessa vida,\\nToda vida, é\\...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Se Eu Não Te Amasse Tanto Assim</td>\n",
       "      <td>/ivete-sangalo/se-eu-nao-te-amasse-tanto-assim...</td>\n",
       "      <td>Meu coração\\nSem direção\\nVoando só por voar\\n...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Céu da Boca</td>\n",
       "      <td>/ivete-sangalo/chupa-toda.html</td>\n",
       "      <td>É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Quando A Chuva Passar</td>\n",
       "      <td>/ivete-sangalo/quando-a-chuva-passar.html</td>\n",
       "      <td>Quando a chuva passar\\n\\nPra quê falar\\nSe voc...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Sorte Grande</td>\n",
       "      <td>/ivete-sangalo/sorte-grande.html</td>\n",
       "      <td>A minha sorte grande foi você cair do céu\\nMin...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ALink                            SName  \\\n",
       "0  /ivete-sangalo/                            Arerê   \n",
       "1  /ivete-sangalo/  Se Eu Não Te Amasse Tanto Assim   \n",
       "2  /ivete-sangalo/                      Céu da Boca   \n",
       "3  /ivete-sangalo/            Quando A Chuva Passar   \n",
       "4  /ivete-sangalo/                     Sorte Grande   \n",
       "\n",
       "                                               SLink  \\\n",
       "0                          /ivete-sangalo/arere.html   \n",
       "1  /ivete-sangalo/se-eu-nao-te-amasse-tanto-assim...   \n",
       "2                     /ivete-sangalo/chupa-toda.html   \n",
       "3          /ivete-sangalo/quando-a-chuva-passar.html   \n",
       "4                   /ivete-sangalo/sorte-grande.html   \n",
       "\n",
       "                                               Lyric language  \n",
       "0  Tudo o que eu quero nessa vida,\\nToda vida, é\\...       pt  \n",
       "1  Meu coração\\nSem direção\\nVoando só por voar\\n...       pt  \n",
       "2  É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...       pt  \n",
       "3  Quando a chuva passar\\n\\nPra quê falar\\nSe voc...       pt  \n",
       "4  A minha sorte grande foi você cair do céu\\nMin...       pt  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "df = pd.read_csv(\"data/lyrics-data.csv\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pdf = pd.read_csv('./data/PoetryFoundationData.csv',quotechar='\"')\n",
    "\n",
    "#pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b23968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALink       object\n",
       "SName       object\n",
       "SLink       object\n",
       "Lyric       object\n",
       "language    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Lyric'] = df[\"Lyric\"].astype(str)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daae86ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp311-cp311-macosx_10_15_x86_64.whl (230.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.1/230.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp311-cp311-macosx_10_9_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jax>=0.3.15\n",
      "  Downloading jax-0.4.6.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-macosx_10_9_x86_64.whl (26.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.24,>=1.22\n",
      "  Downloading numpy-1.23.5-cp311-cp311-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m845.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (23.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.22.1-cp37-abi3-macosx_10_9_universal2.whl (397 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.2/397.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (4.5.0)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m981.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp311-cp311-macosx_10_10_universal2.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow) (1.10.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.3-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m963.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.2)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, pyasn1, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, oauthlib, numpy, markdown, keras, grpcio, google-pasta, gast, cachetools, absl-py, requests-oauthlib, opt-einsum, h5py, google-auth, astunparse, jax, google-auth-oauthlib, tensorboard, tensorflow\n",
      "\u001b[33m  DEPRECATION: wrapt is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "\u001b[33m  DEPRECATION: jax is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for jax ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.16.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 h5py-3.8.0 jax-0.4.6 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.22.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0 werkzeug-2.2.3 wheel-0.40.0 wrapt-1.14.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe29be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Tudo o que eu quero nessa vida,\\nToda vida, é\\...\n",
       "1         Meu coração\\nSem direção\\nVoando só por voar\\n...\n",
       "2         É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...\n",
       "3         Quando a chuva passar\\n\\nPra quê falar\\nSe voc...\n",
       "4         A minha sorte grande foi você cair do céu\\nMin...\n",
       "                                ...                        \n",
       "379926    Chorus\\nHere we stand waiting on the plain\\nDa...\n",
       "379927    I nearly disappeared into the mouth of a croco...\n",
       "379928    Amambuka, amambuka azothengisa izwe lakithi, i...\n",
       "379929    Sweat in the heat for days on end\\nwaiting for...\n",
       "379930    Here we stand on the edge of the day\\nFaces me...\n",
       "Name: Lyric, Length: 379931, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Lyric\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0b89d",
   "metadata": {},
   "source": [
    "## Cleansing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed1978a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/c__4t6z925x17dp2mvhb93sr0000gn/T/ipykernel_11752/1661546544.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  return pd.Series(res)\n",
      "/var/folders/tm/c__4t6z925x17dp2mvhb93sr0000gn/T/ipykernel_11752/1661546544.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  return pd.Series(res)\n",
      "/var/folders/tm/c__4t6z925x17dp2mvhb93sr0000gn/T/ipykernel_11752/1661546544.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  return pd.Series(res)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "      <th>single_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Arerê</td>\n",
       "      <td>/ivete-sangalo/arere.html</td>\n",
       "      <td>Tudo o que eu quero nessa vida,\\nToda vida, é\\...</td>\n",
       "      <td>pt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Se Eu Não Te Amasse Tanto Assim</td>\n",
       "      <td>/ivete-sangalo/se-eu-nao-te-amasse-tanto-assim...</td>\n",
       "      <td>Meu coração\\nSem direção\\nVoando só por voar\\n...</td>\n",
       "      <td>pt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Céu da Boca</td>\n",
       "      <td>/ivete-sangalo/chupa-toda.html</td>\n",
       "      <td>É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...</td>\n",
       "      <td>pt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Quando A Chuva Passar</td>\n",
       "      <td>/ivete-sangalo/quando-a-chuva-passar.html</td>\n",
       "      <td>Quando a chuva passar\\n\\nPra quê falar\\nSe voc...</td>\n",
       "      <td>pt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Sorte Grande</td>\n",
       "      <td>/ivete-sangalo/sorte-grande.html</td>\n",
       "      <td>A minha sorte grande foi você cair do céu\\nMin...</td>\n",
       "      <td>pt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ALink                            SName  \\\n",
       "0  /ivete-sangalo/                            Arerê   \n",
       "1  /ivete-sangalo/  Se Eu Não Te Amasse Tanto Assim   \n",
       "2  /ivete-sangalo/                      Céu da Boca   \n",
       "3  /ivete-sangalo/            Quando A Chuva Passar   \n",
       "4  /ivete-sangalo/                     Sorte Grande   \n",
       "\n",
       "                                               SLink  \\\n",
       "0                          /ivete-sangalo/arere.html   \n",
       "1  /ivete-sangalo/se-eu-nao-te-amasse-tanto-assim...   \n",
       "2                     /ivete-sangalo/chupa-toda.html   \n",
       "3          /ivete-sangalo/quando-a-chuva-passar.html   \n",
       "4                   /ivete-sangalo/sorte-grande.html   \n",
       "\n",
       "                                               Lyric language single_text  \n",
       "0  Tudo o que eu quero nessa vida,\\nToda vida, é\\...       pt              \n",
       "1  Meu coração\\nSem direção\\nVoando só por voar\\n...       pt              \n",
       "2  É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...       pt              \n",
       "3  Quando a chuva passar\\n\\nPra quê falar\\nSe voc...       pt              \n",
       "4  A minha sorte grande foi você cair do céu\\nMin...       pt              "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_text(x):\n",
    "\n",
    "   text = x['Lyric']\n",
    "\n",
    "    \n",
    "   sections = str(text).split()\n",
    "\n",
    "   keys = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n",
    "\n",
    "   lyrics = str()\n",
    "\n",
    "   single_text = []\n",
    "\n",
    "   res = {}\n",
    "\n",
    "   for s in sections:\n",
    "\n",
    "       key = s[s.find('[') + 1:s.find(']')].strip()\n",
    "\n",
    "       if ':' in key:\n",
    "\n",
    "           key = key[:key.find(':')]\n",
    "          \n",
    "\n",
    "       if key in keys:\n",
    "\n",
    "           single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\\\n') if len(x) > 1]\n",
    "          \n",
    "\n",
    "       res['single_text'] =  ' \\n '.join(single_text)\n",
    "\n",
    "   return pd.Series(res)\n",
    "\n",
    "\n",
    "df = df.join( df.apply(split_text, axis=1))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a9583c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoem\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([l\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mtranslate(translator) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(l)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      3\u001b[0m pdf\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "df['single_text'] = df['Poem'].apply(lambda x: ' \\n '.join([l.lower().strip().translate(translator) for l in x.splitlines() if len(l)>0]))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e097f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = pd.DataFrame( df['single_text'] )\n",
    "\n",
    "#sum_df = sum_df.append(pd.DataFrame( pdf['single_text'] ))\n",
    "\n",
    "sum_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724bca1e",
   "metadata": {},
   "source": [
    "## Creating a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db04e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  14578\n",
      "Words with less than 7 appearances: 50\n",
      "Words with more than 7 appearances: 7\n",
      "Valid sequences of size 5: 14174\n",
      "[['\\n', 'chorus', '\\n', 'chorus', 'chorus'], ['chorus', '\\n', 'chorus', 'chorus', '\\n'], ['\\n', 'chorus', '\\n', 'chorus', 'chorus']]\n"
     ]
    }
   ],
   "source": [
    "text_as_list = []\n",
    "\n",
    "frequencies = {}\n",
    "\n",
    "uncommon_words = set()\n",
    "\n",
    "MIN_FREQUENCY = 7\n",
    "\n",
    "MIN_SEQ = 5\n",
    "\n",
    "BATCH_SIZE =  32\n",
    "\n",
    "\n",
    "def extract_text(text):\n",
    "\n",
    "   global text_as_list\n",
    "\n",
    "   text_as_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "\n",
    "\n",
    "sum_df['single_text'].apply( extract_text )\n",
    "\n",
    "print('Total words: ', len(text_as_list))\n",
    "\n",
    "\n",
    "for w in text_as_list:\n",
    "\n",
    "   frequencies[w] = frequencies.get(w, 0) + 1\n",
    "  \n",
    "\n",
    "uncommon_words = set([key for key in frequencies.keys() if frequencies[key] < MIN_FREQUENCY])\n",
    "\n",
    "words = sorted(set([key for key in frequencies.keys() if frequencies[key] >= MIN_FREQUENCY]))\n",
    "\n",
    "\n",
    "num_words = len(words)\n",
    "\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "\n",
    "print('Words with less than {} appearances: {}'.format( MIN_FREQUENCY, len(uncommon_words)))\n",
    "\n",
    "print('Words with more than {} appearances: {}'.format( MIN_FREQUENCY, len(words)))\n",
    "\n",
    "\n",
    "valid_seqs = []\n",
    "\n",
    "end_seq_words = []\n",
    "\n",
    "for i in range(len(text_as_list) - MIN_SEQ ):\n",
    "\n",
    "   end_slice = i + MIN_SEQ + 1\n",
    "\n",
    "   if len( set(text_as_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n",
    "\n",
    "       valid_seqs.append(text_as_list[i: i + MIN_SEQ])\n",
    "\n",
    "       end_seq_words.append(text_as_list[i + MIN_SEQ])\n",
    "      \n",
    "\n",
    "print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.02, random_state=42)\n",
    "\n",
    "print(X_train[2:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2223452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for fit and evaluate\n",
    "\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "\n",
    "   index = 0\n",
    "\n",
    "   while True:\n",
    "\n",
    "       x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n",
    "\n",
    "       y = np.zeros((batch_size), dtype=np.int32)\n",
    "\n",
    "       for i in range(batch_size):\n",
    "\n",
    "           for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "\n",
    "               x[i, t] = word_indices[w]\n",
    "\n",
    "           y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
    "\n",
    "           index = index + 1\n",
    "\n",
    "       yield x, y\n",
    "\n",
    "\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "\n",
    "   # helper function to sample an index from a probability array\n",
    "\n",
    "   preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "   preds = np.log(preds) / temperature\n",
    "\n",
    "   exp_preds = np.exp(preds)\n",
    "\n",
    "   preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "   probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "   return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "\n",
    "   # Function invoked at end of each epoch. Prints generated text.\n",
    "\n",
    "   examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "\n",
    "   # Randomly pick a seed sequence\n",
    "\n",
    "   seed_index = np.random.randint(len(X_train+X_test))\n",
    "\n",
    "   seed = (X_train+X_test)[seed_index]\n",
    "\n",
    "\n",
    "   for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "\n",
    "       sentence = seed\n",
    "\n",
    "       examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "\n",
    "       examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "\n",
    "       examples_file.write(' '.join(sentence))\n",
    "\n",
    "\n",
    "       for i in range(50):\n",
    "\n",
    "           x_pred = np.zeros((1, MIN_SEQ))\n",
    "\n",
    "           for t, word in enumerate(sentence):\n",
    "\n",
    "               x_pred[0, t] = word_indices[word]\n",
    "\n",
    "\n",
    "           preds = model.predict(x_pred, verbose=0)[0]\n",
    "\n",
    "           next_index = sample(preds, diversity)\n",
    "\n",
    "           next_word = indices_word[next_index]\n",
    "\n",
    "\n",
    "           sentence = sentence[1:]\n",
    "\n",
    "           sentence.append(next_word)\n",
    "\n",
    "\n",
    "           examples_file.write(\" \"+next_word)\n",
    "\n",
    "       examples_file.write('\\n')\n",
    "\n",
    "   examples_file.write('='*80 + '\\n')\n",
    "\n",
    "   examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0df0413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "   print('Build model...')\n",
    "\n",
    "   model = Sequential()\n",
    "\n",
    "   model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "\n",
    "   model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "   model.add(Dense(len(words)))\n",
    "\n",
    "   model.add(Activation('softmax'))\n",
    "\n",
    "   return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaf88186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 18:29:16.774044: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-27 18:29:16.775271: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-27 18:29:16.776926: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-03-27 18:29:16.914065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-03-27 18:29:16.957108: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-27 18:29:16.958422: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-27 18:29:16.959541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model()\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/LSTM_LYRICS-epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-words%d-sequence%d-minfreq%d-loss\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-acc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-val_loss\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-val_acc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(words),\u001b[38;5;250m \u001b[39mMIN_SEQ,\u001b[38;5;250m \u001b[39mMIN_FREQUENCY)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(file_path, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m print_callback \u001b[38;5;241m=\u001b[39m LambdaCallback(on_epoch_end\u001b[38;5;241m=\u001b[39mon_epoch_end)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "file_path = f\"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}{(len(words), MIN_SEQ, MIN_FREQUENCY)}\"\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "\n",
    "examples_file = open('examples.txt', \"w\")\n",
    "\n",
    "model.fit(generator(X_train, y_train, BATCH_SIZE),\n",
    "\n",
    "                   steps_per_epoch=int(len(valid_seqs)/BATCH_SIZE) + 1,\n",
    "\n",
    "                   epochs=20,\n",
    "\n",
    "                   callbacks=callbacks_list,\n",
    "\n",
    "                   validation_data=generator(X_test, y_train, BATCH_SIZE),\n",
    "\n",
    "                   validation_steps=int(len(y_train)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103779e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
