{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3d0169",
   "metadata": {},
   "source": [
    "## Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "df = pd.read_csv(\"./data/lyrics.csv\", sep=\"\\t\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "pdf = pd.read_csv('./data/PoetryFoundationData.csv',quotechar='\"')\n",
    "\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daae86ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.5.3-cp310-cp310-macosx_10_9_x86_64.whl (12.0 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Applications/miniconda3/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Applications/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Applications/miniconda3/lib/python3.10/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0b89d",
   "metadata": {},
   "source": [
    "## Cleansing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed1978a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m        res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(single_text)\n\u001b[1;32m     31\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries(res)\n\u001b[0;32m---> 34\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mjoin( df\u001b[38;5;241m.\u001b[39mapply(split_text, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     36\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def split_text(x):\n",
    "\n",
    "   text = x['lyrics']\n",
    "\n",
    "   sections = text.split('\\\\n\\\\n')\n",
    "\n",
    "   keys = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n",
    "\n",
    "   lyrics = str()\n",
    "\n",
    "   single_text = []\n",
    "\n",
    "   res = {}\n",
    "\n",
    "   for s in sections:\n",
    "\n",
    "       key = s[s.find('[') + 1:s.find(']')].strip()\n",
    "\n",
    "       if ':' in key:\n",
    "\n",
    "           key = key[:key.find(':')]\n",
    "          \n",
    "\n",
    "       if key in keys:\n",
    "\n",
    "           single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\\\n') if len(x) > 1]\n",
    "          \n",
    "\n",
    "       res['single_text'] =  ' \\n '.join(single_text)\n",
    "\n",
    "   return pd.Series(res)\n",
    "\n",
    "\n",
    "df = df.join( df.apply(split_text, axis=1))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a9583c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoem\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([l\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mtranslate(translator) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(l)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      3\u001b[0m pdf\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "pdf['single_text'] = pdf['Poem'].apply(lambda x: ' \\n '.join([l.lower().strip().translate(translator) for l in x.splitlines() if len(l)>0]))\n",
    "\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e097f1f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sum_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame( df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle_text\u001b[39m\u001b[38;5;124m'\u001b[39m] )\n\u001b[1;32m      3\u001b[0m sum_df \u001b[38;5;241m=\u001b[39m sum_df\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame( pdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle_text\u001b[39m\u001b[38;5;124m'\u001b[39m] ))\n\u001b[1;32m      5\u001b[0m sum_df\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "sum_df = pd.DataFrame( df['single_text'] )\n",
    "\n",
    "sum_df = sum_df.append(pd.DataFrame( pdf['single_text'] ))\n",
    "\n",
    "sum_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724bca1e",
   "metadata": {},
   "source": [
    "## Creating a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db04e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_list = []\n",
    "\n",
    "frequencies = {}\n",
    "\n",
    "uncommon_words = set()\n",
    "\n",
    "MIN_FREQUENCY = 7\n",
    "\n",
    "MIN_SEQ = 5\n",
    "\n",
    "BATCH_SIZE =  32\n",
    "\n",
    "\n",
    "def extract_text(text):\n",
    "\n",
    "   global text_as_list\n",
    "\n",
    "   text_as_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "\n",
    "\n",
    "sum_df['single_text'].apply( extract_text )\n",
    "\n",
    "print('Total words: ', len(text_as_list))\n",
    "\n",
    "\n",
    "for w in text_as_list:\n",
    "\n",
    "   frequencies[w] = frequencies.get(w, 0) + 1\n",
    "  \n",
    "\n",
    "uncommon_words = set([key for key in frequencies.keys() if frequencies[key] < MIN_FREQUENCY])\n",
    "\n",
    "words = sorted(set([key for key in frequencies.keys() if frequencies[key] >= MIN_FREQUENCY]))\n",
    "\n",
    "\n",
    "num_words = len(words)\n",
    "\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "\n",
    "print('Words with less than {} appearances: {}'.format( MIN_FREQUENCY, len(uncommon_words)))\n",
    "\n",
    "print('Words with more than {} appearances: {}'.format( MIN_FREQUENCY, len(words)))\n",
    "\n",
    "\n",
    "valid_seqs = []\n",
    "\n",
    "end_seq_words = []\n",
    "\n",
    "for i in range(len(text_as_list) - MIN_SEQ ):\n",
    "\n",
    "   end_slice = i + MIN_SEQ + 1\n",
    "\n",
    "   if len( set(text_as_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n",
    "\n",
    "       valid_seqs.append(text_as_list[i: i + MIN_SEQ])\n",
    "\n",
    "       end_seq_words.append(text_as_list[i + MIN_SEQ])\n",
    "      \n",
    "\n",
    "print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.02, random_state=42)\n",
    "\n",
    "print(X_train[2:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2223452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for fit and evaluate\n",
    "\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "\n",
    "   index = 0\n",
    "\n",
    "   while True:\n",
    "\n",
    "       x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n",
    "\n",
    "       y = np.zeros((batch_size), dtype=np.int32)\n",
    "\n",
    "       for i in range(batch_size):\n",
    "\n",
    "           for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "\n",
    "               x[i, t] = word_indices[w]\n",
    "\n",
    "           y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
    "\n",
    "           index = index + 1\n",
    "\n",
    "       yield x, y\n",
    "\n",
    "\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "\n",
    "   # helper function to sample an index from a probability array\n",
    "\n",
    "   preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "   preds = np.log(preds) / temperature\n",
    "\n",
    "   exp_preds = np.exp(preds)\n",
    "\n",
    "   preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "   probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "   return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "\n",
    "   # Function invoked at end of each epoch. Prints generated text.\n",
    "\n",
    "   examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "\n",
    "   # Randomly pick a seed sequence\n",
    "\n",
    "   seed_index = np.random.randint(len(X_train+X_test))\n",
    "\n",
    "   seed = (X_train+X_test)[seed_index]\n",
    "\n",
    "\n",
    "   for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "\n",
    "       sentence = seed\n",
    "\n",
    "       examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "\n",
    "       examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "\n",
    "       examples_file.write(' '.join(sentence))\n",
    "\n",
    "\n",
    "       for i in range(50):\n",
    "\n",
    "           x_pred = np.zeros((1, MIN_SEQ))\n",
    "\n",
    "           for t, word in enumerate(sentence):\n",
    "\n",
    "               x_pred[0, t] = word_indices[word]\n",
    "\n",
    "\n",
    "           preds = model.predict(x_pred, verbose=0)[0]\n",
    "\n",
    "           next_index = sample(preds, diversity)\n",
    "\n",
    "           next_word = indices_word[next_index]\n",
    "\n",
    "\n",
    "           sentence = sentence[1:]\n",
    "\n",
    "           sentence.append(next_word)\n",
    "\n",
    "\n",
    "           examples_file.write(\" \"+next_word)\n",
    "\n",
    "       examples_file.write('\\n')\n",
    "\n",
    "   examples_file.write('='*80 + '\\n')\n",
    "\n",
    "   examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "   print('Build model...')\n",
    "\n",
    "   model = Sequential()\n",
    "\n",
    "   model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "\n",
    "   model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "   model.add(Dense(len(words)))\n",
    "\n",
    "   model.add(Activation('softmax'))\n",
    "\n",
    "   return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf88186",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "\n",
    "           \"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n",
    "\n",
    "           (len(words), MIN_SEQ, MIN_FREQUENCY)\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "\n",
    "examples_file = open('examples.txt', \"w\")\n",
    "\n",
    "model.fit(generator(X_train, y_train, BATCH_SIZE),\n",
    "\n",
    "                   steps_per_epoch=int(len(valid_seqs)/BATCH_SIZE) + 1,\n",
    "\n",
    "                   epochs=20,\n",
    "\n",
    "                   callbacks=callbacks_list,\n",
    "\n",
    "                   validation_data=generator(X_test, y_train, BATCH_SIZE),\n",
    "\n",
    "                   validation_steps=int(len(y_train)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103779e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
